{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project\n",
    "\n",
    "## Scott Scheraga  7/24/2020\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Worker\n",
      "Worker\n",
      "Worker\n",
      "Worker\n",
      "Worker\n",
      "Worker\n",
      "Worker\n",
      "Worker\n",
      "Worker\n",
      "Worker\n",
      "Worker\n",
      "Worker\n",
      "Worker\n",
      "Worker\n",
      "Worker\n",
      "Worker"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WorkerWorker\n",
      "\n",
      "Worker\n",
      "Worker\n"
     ]
    }
   ],
   "source": [
    "#Code is largely built off of https://keras.io/examples/rl/ddpg_pendulum/\n",
    "#HER code is inspired by pybullet code at https://github.com/buntyke/her/blob/master/ddpg_her.py\n",
    "\n",
    "#from gym.envs.registration import registry, make, spec, register\n",
    "#python -m pybullet_envs.examples.enjoy_TF_HalfCheetahBulletEnv_v0_2017may\n",
    "\n",
    "\n",
    "import gym\n",
    "import pybullet_envs\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from gym import wrappers\n",
    "from IPython import display\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "%matplotlib inline\n",
    "import math \n",
    "\n",
    "import os\n",
    "import gym\n",
    "from gym import utils\n",
    "from gym.envs import mujoco\n",
    "import mujoco_py\n",
    "import cv2\n",
    "from gym.envs.robotics import fetch_env\n",
    "import threading\n",
    "threading.activeCount()\n",
    "\n",
    "\n",
    "import multiprocessing\n",
    "\n",
    "def worker():\n",
    "    \"\"\"worker function\"\"\"\n",
    "    print ('Worker')\n",
    "    return\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    jobs = []\n",
    "    for i in range(20):\n",
    "        p = multiprocessing.Process(target=worker)\n",
    "        jobs.append(p)\n",
    "        p.start()\n",
    "        \n",
    "threading.activeCount()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obs space\n",
      "Dict(achieved_goal:Box(3,), desired_goal:Box(3,), observation:Box(25,))\n",
      "OrderedDict([('achieved_goal', array([ 1.2748761 , -0.7461606 , -0.35350913], dtype=float32)), ('desired_goal', array([1.008264  , 2.0391502 , 0.01420115], dtype=float32)), ('observation', array([ 0.9527221 ,  1.060209  , -0.18586233, -0.06924193,  0.12329742,\n",
      "        0.43136293,  1.7060128 ,  0.8749693 ,  1.1029812 , -0.08645099,\n",
      "       -1.5477712 , -0.2621987 , -0.5607177 , -1.7048308 ,  0.30761853,\n",
      "        0.7718381 , -0.03193387,  0.04633267, -0.4844297 , -0.23542917,\n",
      "       -1.5805008 , -0.01176037,  0.5289578 ,  0.21803728, -0.8666665 ],\n",
      "      dtype=float32))])\n",
      " \n",
      " \n",
      "Action space\n",
      "Box(4,)\n",
      "[ 0.552771   -0.04700796  0.9259032   0.5628956 ]\n",
      "numgoals= 3\n",
      " \n",
      " \n",
      "Size of State Space ->  25\n",
      "Size of Action Space ->  4\n",
      "Max Value of Action ->  1.0\n",
      "Min Value of Action ->  -1.0\n",
      " \n",
      " \n",
      " \n"
     ]
    }
   ],
   "source": [
    "\n",
    "#env = gym.make('Pendulum-v0')\n",
    "#env = gym.make('CartPole-v1')\n",
    "#env = gym.make('HalfCheetahBulletEnv-v0')\n",
    "\n",
    "\n",
    "#env = gym.make('FetchPickAndPlace-v1')\n",
    "\n",
    "\n",
    "env = gym.make('FetchPush-v1')\n",
    "#env = gym.make('FetchReach-v1')\n",
    "\n",
    "#env = gym.make('Reacher-v2')\n",
    "\n",
    "#Env information at:\n",
    "# https://medium.com/@Amritpal001/intro-to-robotics-in-openai-fetch-reach-env-automating-robotics-with-reinforcement-learning-part-2b7452f3a5e9\n",
    "\n",
    "print(\"Obs space\")\n",
    "print(env.observation_space) #.shape\n",
    "print(env.observation_space.sample())\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "print(\"Action space\")\n",
    "print(env.action_space) #.shape\n",
    "print(env.action_space.sample())\n",
    "num_states = env.observation_space['observation'].shape[0]\n",
    "num_goals = env.observation_space['achieved_goal'].shape[0]\n",
    "print(\"numgoals=\",num_goals)\n",
    "\n",
    "#num_states = env.observation_space.shape[0]\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "#num_states=25\n",
    "print(\"Size of State Space ->  {}\".format(num_states))\n",
    "num_actions = env.action_space.shape[0]\n",
    "print(\"Size of Action Space ->  {}\".format(num_actions))\n",
    "\n",
    "upper_bound = env.action_space.high[0]\n",
    "lower_bound = env.action_space.low[0]\n",
    "\n",
    "print(\"Max Value of Action ->  {}\".format(upper_bound))\n",
    "print(\"Min Value of Action ->  {}\".format(lower_bound))\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "#print(\"initial_state\")\n",
    "#print(env.initial_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfrom gym import utils\\nfrom gym.envs.robotics import fetch_env\\n\\n\\n# Ensure we get the path separator correct on windows\\nMODEL_XML_PATH = os.path.join('fetch', 'reach.xml')\\n\\n\\nclass FetchReachEnv(fetch_env.FetchEnv, utils.EzPickle):\\n    def __init__(self, reward_type='sparse'):\\n        initial_qpos = {\\n            'robot0:slide0': 0.4049,\\n            'robot0:slide1': 0.48,\\n            'robot0:slide2': 0.0,\\n        }\\n        fetch_env.FetchEnv.__init__(\\n            self, MODEL_XML_PATH, has_object=False, block_gripper=True, n_substeps=20,\\n            gripper_extra_height=0.2, target_in_the_air=True, target_offset=0.0,\\n            obj_range=0.15, target_range=0.15, distance_threshold=0.05,\\n            initial_qpos=initial_qpos, reward_type=reward_type)\\n        utils.EzPickle.__init__(self)\\n        \\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fetch reach definition\n",
    "#https://github.com/openai/gym/blob/master/gym/envs/robotics/fetch/reach.py\n",
    "\"\"\"\n",
    "from gym import utils\n",
    "from gym.envs.robotics import fetch_env\n",
    "\n",
    "\n",
    "# Ensure we get the path separator correct on windows\n",
    "MODEL_XML_PATH = os.path.join('fetch', 'reach.xml')\n",
    "\n",
    "\n",
    "class FetchReachEnv(fetch_env.FetchEnv, utils.EzPickle):\n",
    "    def __init__(self, reward_type='sparse'):\n",
    "        initial_qpos = {\n",
    "            'robot0:slide0': 0.4049,\n",
    "            'robot0:slide1': 0.48,\n",
    "            'robot0:slide2': 0.0,\n",
    "        }\n",
    "        fetch_env.FetchEnv.__init__(\n",
    "            self, MODEL_XML_PATH, has_object=False, block_gripper=True, n_substeps=20,\n",
    "            gripper_extra_height=0.2, target_in_the_air=True, target_offset=0.0,\n",
    "            obj_range=0.15, target_range=0.15, distance_threshold=0.05,\n",
    "            initial_qpos=initial_qpos, reward_type=reward_type)\n",
    "        utils.EzPickle.__init__(self)\n",
    "        \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OUActionNoise:\n",
    "    def __init__(self, mean, std_deviation, theta=0.15, dt=1e-2, x_initial=None):\n",
    "        self.theta = theta\n",
    "        self.mean = mean\n",
    "        self.std_dev = std_deviation\n",
    "        self.dt = dt\n",
    "        self.x_initial = x_initial\n",
    "        self.reset()\n",
    "\n",
    "    def __call__(self):\n",
    "        # Formula taken from https://www.wikipedia.org/wiki/Ornstein-Uhlenbeck_process.\n",
    "        x = (\n",
    "            self.x_prev\n",
    "            + self.theta * (self.mean - self.x_prev) * self.dt\n",
    "            + self.std_dev * np.sqrt(self.dt) * np.random.normal(size=self.mean.shape)\n",
    "        )\n",
    "        # Store x into x_prev\n",
    "        # Makes next noise dependent on current one\n",
    "        self.x_prev = x\n",
    "        return x\n",
    "\n",
    "    def reset(self):\n",
    "        if self.x_initial is not None:\n",
    "            self.x_prev = self.x_initial\n",
    "        else:\n",
    "            self.x_prev = np.zeros_like(self.mean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Buffer:\n",
    "    def __init__(self, buffer_capacity=100000, batch_size=256):\n",
    "    \n",
    "        # Number of \"experiences\" to store at max\n",
    "        self.buffer_capacity = buffer_capacity\n",
    "        # Num of tuples to train on.\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # Its tells us num of times record() was called.\n",
    "        self.buffer_counter = 0\n",
    "\n",
    "        # Instead of list of tuples as the exp.replay concept go\n",
    "        # We use different np.arrays for each tuple element\n",
    "        self.state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
    "        self.action_buffer = np.zeros((self.buffer_capacity, num_actions))\n",
    "        self.reward_buffer = np.zeros((self.buffer_capacity, 1))\n",
    "        self.next_state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
    "        self.achieved_goal_buffer = np.zeros((self.buffer_capacity, num_goals))\n",
    "        self.goal_buffer = np.zeros((self.buffer_capacity, num_goals)) \n",
    "\n",
    "    # Takes (s,a,r,s') obervation tuple as input\n",
    "    def record(self, obs_tuple):\n",
    "        # Set index to zero if buffer_capacity is exceeded,\n",
    "        # replacing old records\n",
    "        index = self.buffer_counter % self.buffer_capacity\n",
    "\n",
    "        self.state_buffer[index] = obs_tuple[0]\n",
    "        self.action_buffer[index] = obs_tuple[1]\n",
    "        self.reward_buffer[index] = obs_tuple[2]\n",
    "        self.next_state_buffer[index] = obs_tuple[3]\n",
    "        self.achieved_goal_buffer[index] = obs_tuple[4]\n",
    "        self.goal_buffer[index] = obs_tuple[5]\n",
    "        \n",
    "        \n",
    "        if self.buffer_counter<self.buffer_capacity:\n",
    "            self.buffer_counter += 1\n",
    "            \n",
    "\n",
    "    # We compute the loss and update parameters\n",
    "    def learn(self):\n",
    "        # Get sampling range\n",
    "        record_range = min(self.buffer_counter, self.buffer_capacity)\n",
    "        # Randomly sample indices\n",
    "        batch_indices = np.random.choice(record_range, self.batch_size)\n",
    "\n",
    "        # Convert to tensors\n",
    "        state_batch = tf.convert_to_tensor(self.state_buffer[batch_indices])\n",
    "        action_batch = tf.convert_to_tensor(self.action_buffer[batch_indices])\n",
    "        reward_batch = tf.convert_to_tensor(self.reward_buffer[batch_indices])\n",
    "        reward_batch = tf.cast(reward_batch, dtype=tf.float32)\n",
    "        next_state_batch = tf.convert_to_tensor(self.next_state_buffer[batch_indices])\n",
    "        achieved_goal_batch = tf.convert_to_tensor(self.achieved_goal_buffer[batch_indices])\n",
    "        goal_batch = tf.convert_to_tensor(self.goal_buffer[batch_indices])\n",
    "\n",
    "        # Training and updating Actor & Critic networks.\n",
    "        # See Pseudo Code.\n",
    "        with tf.GradientTape() as tape:\n",
    "            target_actions = target_actor(next_state_batch)\n",
    "            y = reward_batch + gamma * target_critic([next_state_batch, target_actions])\n",
    "            critic_value = critic_model([state_batch, action_batch])\n",
    "            critic_loss = tf.math.reduce_mean(tf.math.square(y - critic_value))\n",
    "\n",
    "        critic_grad = tape.gradient(critic_loss, critic_model.trainable_variables)\n",
    "        critic_optimizer.apply_gradients(\n",
    "            zip(critic_grad, critic_model.trainable_variables)\n",
    "        )\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            actions = actor_model(state_batch)\n",
    "            critic_value = critic_model([state_batch, actions])\n",
    "            # Used `-value` as we want to maximize the value given\n",
    "            # by the critic for our actions\n",
    "            actor_loss = -tf.math.reduce_mean(critic_value)\n",
    "\n",
    "        actor_grad = tape.gradient(actor_loss, actor_model.trainable_variables)\n",
    "        actor_optimizer.apply_gradients(\n",
    "            zip(actor_grad, actor_model.trainable_variables)\n",
    "        )\n",
    "\n",
    "\n",
    "# This update target parameters slowly\n",
    "# Based on rate `tau`, which is much less than one.\n",
    "def update_target(tau):\n",
    "    new_weights = []\n",
    "    target_variables = target_critic.weights\n",
    "    for i, variable in enumerate(critic_model.weights):\n",
    "        new_weights.append(variable * tau + target_variables[i] * (1 - tau))\n",
    "\n",
    "    target_critic.set_weights(new_weights)\n",
    "\n",
    "    new_weights = []\n",
    "    target_variables = target_actor.weights\n",
    "    for i, variable in enumerate(actor_model.weights):\n",
    "        new_weights.append(variable * tau + target_variables[i] * (1 - tau))\n",
    "\n",
    "    target_actor.set_weights(new_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "layersize=400\n",
    "\n",
    "def get_actor():  #makes actor network\n",
    "    # Initialize weights between -3e-3 and 3-e3\n",
    "    last_init = tf.random_uniform_initializer(minval=-0.003, maxval=0.003)\n",
    "\n",
    "    inputs = layers.Input(shape=(num_states,))\n",
    "    out = layers.Dense(layersize, activation=\"relu\")(inputs)\n",
    "    out = layers.BatchNormalization()(out)\n",
    "    out = layers.Dense(layersize, activation=\"relu\")(out)\n",
    "    out = layers.BatchNormalization()(out)\n",
    "    outputs = layers.Dense(num_actions, activation=\"tanh\", kernel_initializer=last_init)(out)\n",
    "    # Our upper bound is 2.0 for Pendulum.\n",
    "    outputs = outputs * upper_bound\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    return model\n",
    " \n",
    "def get_critic():\n",
    "    # State as input\n",
    "    state_input = layers.Input(shape=(num_states))\n",
    "    #state_out = layers.Dense(16, activation=\"relu\")(state_input)#Changed from 16 to 512 in V5\n",
    "    #state_out = layers.BatchNormalization()(state_out)\n",
    "    #state_out = layers.Dense(32, activation=\"relu\")(state_out)#Changed from 32 to 512 in V5\n",
    "    #state_out = layers.BatchNormalization()(state_out) \n",
    "\n",
    "    # Action as input\n",
    "    action_input = layers.Input(shape=(num_actions))\n",
    "    #action_out = layers.Dense(32, activation=\"relu\")(action_input)  #Changed from 32 to 512 in V5\n",
    "    #action_out = layers.BatchNormalization()(action_out)#Changed from 32 to 512 in V5\n",
    "\n",
    " \n",
    "    # Both are passed through seperate layer before concatenating\n",
    "    #concat = layers.Concatenate()([state_out, action_out])\n",
    "    concat = layers.Concatenate()([state_input, action_input])\n",
    "\n",
    "    out = layers.Dense(layersize, activation=\"relu\")(concat)\n",
    "    out = layers.BatchNormalization()(out)\n",
    "    out = layers.Dense(layersize, activation=\"relu\")(out)\n",
    "    out = layers.BatchNormalization()(out)\n",
    "    outputs = layers.Dense(1)(out)\n",
    "\n",
    "    # Outputs single value for give state-action\n",
    "    model = tf.keras.Model([state_input, action_input], outputs)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy(state, noise_object):\n",
    "    sampled_actions = tf.squeeze(actor_model(state))\n",
    "    noise = noise_object()\n",
    "    # Adding noise to action\n",
    "    sampled_actions = sampled_actions.numpy() + noise\n",
    "\n",
    "    # We make sure action is within bounds\n",
    "    legal_action = np.clip(sampled_actions, lower_bound, upper_bound)\n",
    "\n",
    "    return np.squeeze(legal_action)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "std_dev = 0.2\n",
    "ou_noise = OUActionNoise(mean=np.zeros(1), std_deviation=float(std_dev) * np.ones(1))\n",
    "\n",
    "#actor_model = get_actor()\n",
    "#critic_model = get_critic()\n",
    "\n",
    "#target_actor = get_actor()\n",
    "#target_critic = get_critic()\n",
    "#print(actor_model.summary() )\n",
    "#critic_model.summary() \n",
    "\n",
    "# Making the weights equal initially\n",
    "\n",
    "\n",
    "# Learning rate for actor-critic models\n",
    "critic_lr = 0.001  #originally .002\n",
    "actor_lr = 0.001  #originally .001\n",
    "\n",
    "\n",
    "#total_episodes = 100\n",
    "# Discount factor for future rewards\n",
    "gamma = 0.99\n",
    "# Used to update target networks\n",
    "tau = 0.01\n",
    "\n",
    "\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time is:  2020-07-24 13:41:47.799067\n",
      "Episode * 0 *  HER Reward: 0.0 *  Episodic Reward: -50.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-ed965660bb4f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;31m#tf_prev_state = tf.expand_dims(tf.convert_to_tensor(prev_state), 0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf_prev_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mou_noise\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0;31m# Recieve state and reward from environment.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-6e86c8c0202d>\u001b[0m in \u001b[0;36mpolicy\u001b[0;34m(state, noise_object)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoise_object\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0msampled_actions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactor_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mnoise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnoise_object\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# Adding noise to action\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0msampled_actions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msampled_actions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnoise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/csci-7000-rl/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    966\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    967\u001b[0m               self._compute_dtype):\n\u001b[0;32m--> 968\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    969\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/csci-7000-rl/lib/python3.7/site-packages/tensorflow/python/keras/engine/network.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    717\u001b[0m     return self._run_internal_graph(\n\u001b[1;32m    718\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 719\u001b[0;31m         convert_kwargs_to_constants=base_layer_utils.call_context().saving)\n\u001b[0m\u001b[1;32m    720\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcompute_output_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/csci-7000-rl/lib/python3.7/site-packages/tensorflow/python/keras/engine/network.py\u001b[0m in \u001b[0;36m_run_internal_graph\u001b[0;34m(self, inputs, training, mask, convert_kwargs_to_constants)\u001b[0m\n\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m           \u001b[0;31m# Compute outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 888\u001b[0;31m           \u001b[0moutput_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomputed_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    889\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m           \u001b[0;31m# Update tensor_dict.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/csci-7000-rl/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    966\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    967\u001b[0m               self._compute_dtype):\n\u001b[0;32m--> 968\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    969\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/csci-7000-rl/lib/python3.7/site-packages/tensorflow/python/keras/layers/normalization.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training)\u001b[0m\n\u001b[1;32m    882\u001b[0m                                      \u001b[0moffset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    883\u001b[0m                                      \u001b[0mscale\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 884\u001b[0;31m                                      self.epsilon)\n\u001b[0m\u001b[1;32m    885\u001b[0m     \u001b[0;31m# If some components of the shape got lost due to adjustments, fix that.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m     \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/csci-7000-rl/lib/python3.7/site-packages/tensorflow/python/ops/nn_impl.py\u001b[0m in \u001b[0;36mbatch_normalization\u001b[0;34m(x, mean, variance, offset, scale, variance_epsilon, name)\u001b[0m\n\u001b[1;32m   1414\u001b[0m   \"\"\"\n\u001b[1;32m   1415\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"batchnorm\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1416\u001b[0;31m     \u001b[0minv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrsqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariance\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mvariance_epsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1417\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mscale\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1418\u001b[0m       \u001b[0minv\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/csci-7000-rl/lib/python3.7/site-packages/tensorflow/python/ops/variables.py\u001b[0m in \u001b[0;36m_run_op\u001b[0;34m(a, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1070\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1071\u001b[0m       \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1072\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtensor_oper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1073\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1074\u001b[0m     \u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_run_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_oper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/csci-7000-rl/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36mvalue\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    547\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cached_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cached_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_existing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_variable_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/csci-7000-rl/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mcolocate_with\u001b[0;34m(op, ignore_existing)\u001b[0m\n\u001b[1;32m   5132\u001b[0m \u001b[0;31m# only API for those uses to avoid deprecation warning.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5133\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcolocate_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_existing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5134\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_colocate_with_for_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_existing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_existing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/csci-7000-rl/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_colocate_with_for_gradient\u001b[0;34m(op, gradient_uid, ignore_existing)\u001b[0m\n\u001b[1;32m   5109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5110\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_colocate_with_for_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient_uid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_existing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5111\u001b[0;31m   \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5112\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mop\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5113\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"device\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# To store reward history of each episode\n",
    "\n",
    "ep_reward_list = []\n",
    "buffersize=100000\n",
    "buffer = Buffer(buffersize, 128)  \n",
    "actor_model = get_actor()\n",
    "critic_model = get_critic()\n",
    "\n",
    "\n",
    "target_actor = get_actor()\n",
    "target_critic = get_critic()\n",
    "target_actor.set_weights(actor_model.get_weights())\n",
    "target_critic.set_weights(critic_model.get_weights())\n",
    "critic_optimizer = tf.keras.optimizers.Adam(critic_lr)\n",
    "actor_optimizer = tf.keras.optimizers.Adam(actor_lr)\n",
    "\n",
    "\"\"\"\"\"\"\n",
    "\n",
    "HER_active=True\n",
    "K=4 #K is the ratio of HER buffer length to the regular buffer length\n",
    "\n",
    "\"\"\"\n",
    "actor_model.load_weights(\"chet_actor100v8.h5\")\n",
    "critic_model.load_weights(\"chet_critic100v8.h5\")\n",
    "\n",
    "target_actor.load_weights(\"chet_target_actor100v8.h5\")\n",
    "target_critic.load_weights(\"chet_target_critic100v8.h5\")\n",
    "\n",
    "\n",
    "with open('rewardlist400v5.txt', 'r') as filehandle:\n",
    "    for line in filehandle:\n",
    "        # remove linebreak which is the last character of the string\n",
    "        currentPlace = line[:-1]\n",
    "\n",
    "        # add item to the list\n",
    "        ep_reward_list.append(currentPlace)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "#env.render()\n",
    "#cymj.MjRenderContextOffscreen(self.sim, 0)\n",
    "dateTimeObj = datetime.now()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Time is: \", dateTimeObj)\n",
    "\n",
    "\n",
    "for ep in range(40000):\n",
    "    \"\"\"  \n",
    "    if ep % 1000 ==0: \n",
    "        actor_model.save_weights(\"pushHER_actor40K.h5\")\n",
    "        critic_model.save_weights(\"pushHER_critic40K.h5\")\n",
    "\n",
    "        target_actor.save_weights(\"pushHER_target_actor40K.h5\")\n",
    "        target_critic.save_weights(\"pushHER_target_critic40K.h5\")\n",
    "\n",
    "        with open('rewardlistpushHER40K.txt', 'w') as filehandle:\n",
    "            for listitem in ep_reward_list:\n",
    "                filehandle.write('%s\\n' % listitem)\n",
    "                \n",
    "    \"\"\"        \n",
    "    prev_state = env.reset()\n",
    "    \n",
    "    episodic_reward = 0\n",
    "    #if HER_active==True:\n",
    "    epochbuffer = Buffer(50, 128)\n",
    "    epochHERbuffer = Buffer(50, 128)\n",
    "    while True:\n",
    "\n",
    "        tf_prev_state = tf.expand_dims(tf.convert_to_tensor(prev_state['observation']), 0)\n",
    "        #tf_prev_state = tf.expand_dims(tf.convert_to_tensor(prev_state), 0)\n",
    "        \n",
    "        action = policy(tf_prev_state, ou_noise)\n",
    "        # Recieve state and reward from environment.\n",
    "        state, reward, done, info = env.step(action)\n",
    "\n",
    "        #desired_goal\n",
    "        #achieved_goal\n",
    "        buffer.record((prev_state['observation'] ,action, \n",
    "                       reward, state['observation'],state['achieved_goal'],state['desired_goal']))\n",
    "        \n",
    "        episodic_reward += reward\n",
    "        \n",
    "        epochbuffer.record((prev_state['observation'] ,action, \n",
    "                       reward, state['observation'],state['achieved_goal'],state['desired_goal']))\n",
    "        #buffer.record((prev_state, action, reward, state))\n",
    "  \n",
    "        \n",
    "        # End this episode when `done` is True\n",
    "        if done:\n",
    "            break \n",
    "            \n",
    "    if HER_active==True:\n",
    "            tempreward=0\n",
    "                 \n",
    "            for t in range(epochbuffer.buffer_counter):\n",
    "                \n",
    "                for k in range(4):\n",
    "                        #For \"future\" strategy\n",
    "                        strategyindex = np.random.randint(t,epochbuffer.buffer_counter)\n",
    "                        \n",
    "                        #For \"final\" strategy\n",
    "                        #strategyindex = epochbuffer.buffer_counter-1\n",
    "                        \n",
    "                        #For \"random\" strategy\n",
    "                        #strategyindex = np.random.randint(0,epochbuffer.buffer_counter)\n",
    "                        \n",
    "\n",
    "                        stateHER = epochbuffer.state_buffer[t]                 \n",
    "                        actionHER = epochbuffer.action_buffer[t]\n",
    "                        next_stateHER = epochbuffer.next_state_buffer[t]\n",
    "                        achieved_goalHER = epochbuffer.achieved_goal_buffer[t]\n",
    "                        \n",
    "                        goalHER = epochbuffer.achieved_goal_buffer[strategyindex]\n",
    "                        \n",
    "                        rewardHER=env.compute_reward(achieved_goalHER, goalHER, info)\n",
    "                        tempreward+=rewardHER\n",
    "                        buffer.record((stateHER, actionHER, \n",
    "                                         rewardHER, next_stateHER, achieved_goalHER, goalHER))\n",
    "                           \n",
    "        \n",
    "    del epochbuffer  \n",
    "        \n",
    "    buffer.learn()    \n",
    "    update_target(tau)    \n",
    "    prev_state = state    \n",
    "        \n",
    "    ep_reward_list.append(episodic_reward)\n",
    "\n",
    "    # Mean of last 40 episodes\n",
    "    #avg_reward = np.mean(ep_reward_list[-40:])\n",
    "    if ep % 100 ==0:\n",
    "        print(\"Episode * {} *  HER Reward: {} *  Episodic Reward: {}\".format(ep,tempreward, episodic_reward ))\n",
    "    #print(\"buffersize= \",buffer.buffer_counter)\n",
    "    #avg_reward_list.append(avg_reward)\n",
    "    \n",
    "dateTimeObj = datetime.now()\n",
    "print(\"Time is: \", dateTimeObj)\n",
    "\n",
    "# Plotting graph\n",
    "# Episodes versus Avg. Rewards\n",
    "plt.plot(ep_reward_list)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Avg. Episodic Reward\")\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "plt.plot(ep_reward_list)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Avg. Episodic Reward\")\n",
    "plt.show()\n",
    "\n",
    "actor_model.save_weights(\"pushHER_actor.h5\")\n",
    "critic_model.save_weights(\"pushHER_critic.h5\")\n",
    "\n",
    "target_actor.save_weights(\"pushHER_target_actor.h5\")\n",
    "target_critic.save_weights(\"pushHER_target_critic.h5\")\n",
    "\n",
    "with open('rewardlistpushHER.txt', 'w') as filehandle:\n",
    "    for listitem in ep_reward_list:\n",
    "          filehandle.write('%s\\n' % listitem)\n",
    "                \n",
    "\"\"\"             \n",
    "actor_model.load_weights(\"pushHER_actor.h5\")\n",
    "critic_model.load_weights(\"pushHER_critic.h5\")\n",
    "\n",
    "target_actor.load_weights(\"pushHER_target_actor.h5\")\n",
    "target_critic.load_weights(\"pushHER_target_critic.h5\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-845e5b296d0e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rgb_array'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# just update the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mdisplay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgcf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mdisplay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/csci-7000-rl/lib/python3.7/site-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, **kwargs)\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/csci-7000-rl/lib/python3.7/site-packages/gym/envs/robotics/fetch_env.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, width, height)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFetchEnv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/csci-7000-rl/lib/python3.7/site-packages/gym/envs/robotics/robot_env.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, width, height)\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_callback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'rgb_array'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_viewer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m             \u001b[0;31m# window size used for old mujoco-py:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_viewer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_pixels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#actor_model.load_weights(\"chet_actor140v5.h5\")\n",
    "#critic_model.load_weights(\"chet_critic140v5.h5\")\n",
    "\n",
    "#target_actor.load_weights(\"chet_target_actor140v5.h5\")\n",
    "#target_critic.load_weights(\"chet_target_critic140v5.h5\")\n",
    "\n",
    "\n",
    "img = plt.imshow(env.render(mode='rgb_array')) # only call this once\n",
    "success=0\n",
    "for j in range(10):\n",
    "    env.reset()\n",
    "    for j in range(50):\n",
    "        \n",
    "        \n",
    "        img.set_data(env.render(mode='rgb_array')) # just update the data\n",
    "        display.display(plt.gcf())\n",
    "        display.clear_output(wait=True)\n",
    "        \n",
    "        \n",
    "        #env.render()\n",
    "        action = env.action_space.sample()\n",
    "    #env.step(action)  #investigate \n",
    "        state, reward, done, info = env.step(action)\n",
    "        if reward==0:\n",
    "            print(\"reached target at timestep\",i)\n",
    "            success+=1\n",
    "            break\n",
    "print(\"sucesses=\",success)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "actor_model.load_weights(\"chet_actor200v7.h5\")\n",
    "critic_model.load_weights(\"chet_critic200v7.h5\")\n",
    "\n",
    "target_actor.load_weights(\"chet_target_actor200v87.h5\")\n",
    "target_critic.load_weights(\"chet_target_critic200v7.h5\")\n",
    "\n",
    "with open('rewardlist200v8.txt', 'w') as filehandle:\n",
    "    for listitem in ep_reward_list:\n",
    "        filehandle.write('%s\\n' % listitem)\n",
    "\"\"\"      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
