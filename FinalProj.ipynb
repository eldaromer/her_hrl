{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "\n",
    "import os\n",
    "import gym\n",
    "from gym import utils\n",
    "from gym.envs import mujoco\n",
    "from gym.envs.robotics import fetch_env\n",
    "#mujoco.FetchPickAndPlace-v0\n",
    "\n",
    "\n",
    "\n",
    "tf.keras.backend.set_floatx('float64')\n",
    "\n",
    "env = gym.make('FetchPickAndPlace-v1')\n",
    "#env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#https://github.com/openai/gym/blob/master/gym/envs/robotics/fetch/pick_and_place.py\n",
    "\n",
    "# Ensure we get the path separator correct on windows\n",
    "MODEL_XML_PATH = os.path.join('fetch', 'pick_and_place.xml')\n",
    "\n",
    "\n",
    "class FetchPickAndPlaceEnv(fetch_env.FetchEnv, utils.EzPickle):\n",
    "    def __init__(self, reward_type='sparse'):\n",
    "        initial_qpos = {\n",
    "            'robot0:slide0': 0.405,\n",
    "            'robot0:slide1': 0.48,\n",
    "            'robot0:slide2': 0.0,\n",
    "            'object0:joint': [1.25, 0.53, 0.4, 1., 0., 0., 0.],\n",
    "        }\n",
    "        fetch_env.FetchEnv.__init__(\n",
    "            self, MODEL_XML_PATH, has_object=True, block_gripper=False, n_substeps=20,\n",
    "            gripper_extra_height=0.2, target_in_the_air=True, target_offset=0.0,\n",
    "            obj_range=0.15, target_range=0.15, distance_threshold=0.05,\n",
    "            initial_qpos=initial_qpos, reward_type=reward_type)\n",
    "        utils.EzPickle.__init__(self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(tf.keras.Model):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(Actor, self).__init__()\n",
    "        self.Dense1 = tf.keras.layers.Dense(hidden_size, activation='relu')\n",
    "        self.Dense2 = tf.keras.layers.Dense(hidden_size, activation='relu')\n",
    "        self.out = tf.keras.layers.Dense(output_size, activation='tanh')\n",
    "        \n",
    "    def call(self, state):\n",
    "        x = self.Dense1(state)\n",
    "        x = self.Dense2(x)\n",
    "        return self.out(x)\n",
    "    \n",
    "class Critic(tf.keras.Model):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(Critic, self).__init__()\n",
    "        self.Dense1 = tf.keras.layers.Dense(hidden_size, activation='relu')\n",
    "        self.Dense2 = tf.keras.layers.Dense(hidden_size, activation='relu')\n",
    "        self.out = tf.keras.layers.Dense(output_size)\n",
    "        \n",
    "    def call(self, state, actions):\n",
    "        x = tf.concat([state, actions], 1)\n",
    "        x = self.Dense1(x)\n",
    "        x = self.Dense2(x)\n",
    "        return self.out(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import random\n",
    "\n",
    "class ReplayMemory:\n",
    "    def __init__(self, size):\n",
    "        self.buffer = deque(maxlen=size)\n",
    "        \n",
    "    def push(self, state, action, reward, next_state, done, goal):\n",
    "        new_experience = (state, action, [reward], next_state, done, goal)\n",
    "        self.buffer.append(new_experience)\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        state_batch = []\n",
    "        action_batch = []\n",
    "        reward_batch = []\n",
    "        next_state_batch = []\n",
    "        done_batch = []\n",
    "        goal_batch = []\n",
    "        \n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "\n",
    "        for experience in batch:\n",
    "            state, action, reward, next_state, done, goal = experience\n",
    "            state_batch.append(state)\n",
    "            action_batch.append(action)\n",
    "            reward_batch.append(reward)\n",
    "            next_state_batch.append(next_state)\n",
    "            done_batch.append(done)\n",
    "            goal_batch.append(goal)\n",
    "        \n",
    "        return (np.array(state_batch), np.array(action_batch), \n",
    "                np.array(reward_batch), np.array(next_state_batch), \n",
    "                np.array(done_batch),np.array(goal_batch))\n",
    " \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "    \n",
    "# Ornstein-Ulhenbeck Process\n",
    "# Taken from #https://github.com/vitchyr/rlkit/blob/master/rlkit/exploration_strategies/ou_strategy.py\n",
    "class OUNoise(object):\n",
    "    def __init__(self, action_space, mu=0.0, theta=0.15, max_sigma=0.3, min_sigma=0.3, decay_period=100000):\n",
    "        self.mu           = mu\n",
    "        self.theta        = theta\n",
    "        self.sigma        = max_sigma\n",
    "        self.max_sigma    = max_sigma\n",
    "        self.min_sigma    = min_sigma\n",
    "        self.decay_period = decay_period\n",
    "        self.action_dim   = action_space.shape[0]\n",
    "        self.low          = action_space.low\n",
    "        self.high         = action_space.high\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.state = np.ones(self.action_dim) * self.mu\n",
    "        \n",
    "    def evolve_state(self):\n",
    "        x  = self.state\n",
    "        dx = self.theta * (self.mu - x) + self.sigma * np.random.randn(self.action_dim)\n",
    "        self.state = x + dx\n",
    "        return self.state\n",
    "    \n",
    "    def get_action(self, action, t=0):\n",
    "        ou_state = self.evolve_state()\n",
    "        self.sigma = self.max_sigma - (self.max_sigma - self.min_sigma) * min(1.0, t / self.decay_period)\n",
    "        return np.clip(action + ou_state, self.low, self.high)\n",
    "    \n",
    "# https://github.com/openai/gym/blob/master/gym/core.py\n",
    "class NormalizedEnv(gym.ActionWrapper):\n",
    "    \"\"\" Wrap action \"\"\"\n",
    "\n",
    "    def action(self, action):\n",
    "        act_k = (self.action_space.high - self.action_space.low)/ 2.\n",
    "        act_b = (self.action_space.high + self.action_space.low)/ 2.\n",
    "        return act_k * action + act_b\n",
    "\n",
    "    def reverse_action(self, action):\n",
    "        act_k_inv = 2./(self.action_space.high - self.action_space.low)\n",
    "        act_b = (self.action_space.high + self.action_space.low)/ 2.\n",
    "        return act_k_inv * (action - act_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import pybullet_envs\n",
    "import gym\n",
    "from gym import utils\n",
    "from gym.envs.robotics import fetch_env\n",
    "gym.logger.set_level(40)\n",
    "\"\"\"\n",
    "#env = NormalizedEnv(gym.make('HalfCheetahBulletEnv-v0'))\n",
    "#env = NormalizedEnv(gym.make('FetchPickAndPlace-v0'))\n",
    "env = gym.make('FetchPickAndPlace-v1')\n",
    "HER_active=False\n",
    "\n",
    "env.render()\n",
    "num_ep = 200\n",
    "batch_size = 128\n",
    "memory_size = 50000\n",
    "tau = 1e-2\n",
    "gamma = 0.99\n",
    "\n",
    "# Initialize Actor and Critic Networks and Target Networks and Optimizers\n",
    "actor = Actor(400, 6)\n",
    "critic = Critic(400, 6)\n",
    "\n",
    "actor_target = Actor(400, 6)\n",
    "critic_target = Critic(400, 6)\n",
    "\n",
    "actor_target.set_weights(actor.get_weights())\n",
    "critic_target.set_weights(critic.get_weights())\n",
    "\n",
    "critic_optimizer = tf.keras.optimizers.Adam(lr=0.001)\n",
    "actor_optimizer = tf.keras.optimizers.Adam(lr=0.0001)\n",
    "\n",
    "# Initialize Replay Memory\n",
    "memory = ReplayMemory(memory_size)\n",
    "# Initialize HER Replay Memory\n",
    "HER_memory = ReplayMemory(memory_size)\n",
    "\n",
    "# Initialize Noise\n",
    "noise = OUNoise(env.action_space)\n",
    "\n",
    "reward_hist = []\n",
    "\n",
    "for ep in range(num_ep):\n",
    "    # Initialize Random Process\n",
    "    noise.reset()\n",
    "    \n",
    "    # Get Initial Observation\n",
    "    observation = env.reset()\n",
    "    \n",
    "    # Reset Stats\n",
    "    total_reward = 0\n",
    "    reward_max = 0\n",
    "    stale = 0\n",
    "    \n",
    "    for t in range(10000):\n",
    "        # Select action from policy and noise\n",
    "        action = actor(np.array([observation,]))[0]\n",
    "        action = noise.get_action(action, t)\n",
    "        \n",
    "        # Execute action and observe reward/new state\n",
    "        new_observation, reward, done, info = env.step(action)\n",
    "        #Scott: goal is not returned with env.step. \n",
    "        \n",
    "        # Store transition is replay memory\n",
    "        memory.push(observation, action, reward, new_observation, done, goal)\n",
    "        \n",
    "        \n",
    "            \n",
    "        \n",
    "        # Sample minibatch and perform update\n",
    "        if len(memory) > batch_size:\n",
    "        \n",
    "            if HER_active==True:\n",
    "            #inspired by https://github.com/buntyke/her/blob/master/ddpg_her.py    \n",
    "            \n",
    "                # get K future states per time step\n",
    "                for t in range(len(memory)):\n",
    "                    for n in range(K): # K = args ratio WE NEED TO SET K!\n",
    "                    # get random future t\n",
    "                        future = np.random.randint(t, len(memory))\n",
    "\n",
    "                        # get new goal at t_future\n",
    "                        #Scott: NEED TO FIX memory location references!!!\n",
    "                        HER_state = memory[t][0]\n",
    "                        HER_action = memory[t][1]\n",
    "                        HER_next_state = memory[t][3]\n",
    "                        HER_goal = memory[future][3] \n",
    "                        HER_done, HER_reward = env.reward_func(HER_next_state, HER_goal)\n",
    "\n",
    "                        # add new experience to her\n",
    "                        HER_memory.add(HER_state, HER_action, HER_reward, \n",
    "                                                          HER_next_state, HER_done, HER_goal)\n",
    "               \n",
    "                (state_batch, action_batch, reward_batch, next_state_batch, \n",
    "                 done_batch, goal_batch) = HER_memory.sample(batch_size) \n",
    "                \n",
    "                #clear HER memory\n",
    "                HER_memory = ReplayMemory(memory_size)\n",
    "            \n",
    "            (state_batch, action_batch, reward_batch, next_state_batch, \n",
    "             done_batch, goal_batch) = memory.sample(batch_size)\n",
    "            \n",
    "            # Update Critic\n",
    "            with tf.GradientTape() as tape:\n",
    "                next_act = actor_target(next_state_batch)\n",
    "                next_q = critic_target(next_state_batch, next_act)\n",
    "                y = reward_batch + gamma * next_q\n",
    "                cur_q = critic(state_batch, action_batch)\n",
    "                loss = tf.reduce_mean((y-cur_q)**2)\n",
    "            grad = tape.gradient(loss, critic.trainable_variables)\n",
    "            critic_optimizer.apply_gradients(zip(grad, critic.trainable_variables))\n",
    "            \n",
    "            # Update Actor\n",
    "            with tf.GradientTape() as tape:\n",
    "                new_q = critic(state_batch, actor(state_batch))\n",
    "                loss = -1 * tf.reduce_mean(new_q)\n",
    "            grad = tape.gradient(loss, actor.trainable_variables)\n",
    "            actor_optimizer.apply_gradients(zip(grad, actor.trainable_variables))\n",
    "            \n",
    "            # Update target networks\n",
    "            actor_target.set_weights(tau * np.array(actor.get_weights()) + (1 - tau) * np.array(\n",
    "                actor_target.get_weights()))\n",
    "            critic_target.set_weights(tau * np.array(critic.get_weights()) + (1 - tau) * np.array(\n",
    "                critic_target.get_weights()))\n",
    "            \n",
    "        \n",
    "        total_reward += reward\n",
    "        if done:\n",
    "            #print(\"stepcounter= \",t)\n",
    "            break\n",
    "        #if total_reward > reward_max:\n",
    "        #    reward_max = total_reward\n",
    "        #else:\n",
    "        #    stale += 1\n",
    "        #if stale > 500:\n",
    "        #    break\n",
    "        observation = new_observation\n",
    "        \n",
    "    print(\"Episode {} completed: {}\".format(ep+1, total_reward))\n",
    "    reward_hist.append(total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/scott/anaconda3/envs/csci-7000-rl/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:1817: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: ./assets\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7efbcc5d8690>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "actor.save('./')\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_xlabel('Episode')\n",
    "ax.set_ylabel('Reward')\n",
    "ax.set_title('Episode vs.Reward')\n",
    "\n",
    "ax.plot(list(range(1, len(reward_hist)+1)), reward_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-98a1db0d953d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m# Select action from policy and noise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m# Execute action and observe reward/new state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/csci-7000-rl/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    966\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    967\u001b[0m               self._compute_dtype):\n\u001b[0;32m--> 968\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    969\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/csci-7000-rl/lib/python3.7/site-packages/tensorflow/python/keras/saving/saved_model/utils.py\u001b[0m in \u001b[0;36mreturn_outputs_and_add_losses\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minputs_arg_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minputs_arg_index\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m     \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m     \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/csci-7000-rl/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/csci-7000-rl/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    616\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 618\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    619\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[0;32m~/anaconda3/envs/csci-7000-rl/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/csci-7000-rl/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1664\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1665\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1667\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/csci-7000-rl/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1688\u001b[0m     \u001b[0;31m# Copy saveable status of function's graph to current FuncGraph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1689\u001b[0;31m     \u001b[0mdefault_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1690\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdefault_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilding_function\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1691\u001b[0m       \u001b[0mdefault_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmark_as_unsaveable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_func_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaving_errors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/csci-7000-rl/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mget_default_graph\u001b[0;34m()\u001b[0m\n\u001b[1;32m   5814\u001b[0m     \u001b[0mThe\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mGraph\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mbeing\u001b[0m \u001b[0mused\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mcurrent\u001b[0m \u001b[0mthread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5815\u001b[0m   \"\"\"\n\u001b[0;32m-> 5816\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_default_graph_stack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5817\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/csci-7000-rl/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mget_default\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   5371\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mget_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5372\u001b[0m     \u001b[0;34m\"\"\"Override that returns a global default if the stack is empty.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5373\u001b[0;31m     \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_DefaultGraphStack\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5374\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5375\u001b[0m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_GetGlobalDefaultGraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pybullet_envs\n",
    "import pybullet as p\n",
    "import time\n",
    "from gym.wrappers import Monitor\n",
    "gym.logger.set_level(40)\n",
    "\n",
    "actor = tf.keras.models.load_model('./')\n",
    "\n",
    "env = NormalizedEnv(gym.make('HalfCheetahBulletEnv-v0'))\n",
    "env.spec.timestep_limit = 9000\n",
    "\n",
    "\n",
    "observation = env.reset()\n",
    "reward_hist = []\n",
    "\n",
    "for ep in range(200):\n",
    "    total_reward = 0\n",
    "    observation = env.reset()\n",
    "    for t in range(10000):\n",
    "        # Select action from policy and noise\n",
    "        action = actor(np.array([observation,]))[0]\n",
    "\n",
    "        # Execute action and observe reward/new state\n",
    "        new_observation, reward, done, info = env.step(action)\n",
    "        total_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "        observation = new_observation\n",
    "    reward_hist.append(total_reward)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-3cced6ba90d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_subplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m121\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_xlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Episode'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_ylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Reward'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_title\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Episode vs.Reward'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(121)\n",
    "ax.set_xlabel('Episode')\n",
    "ax.set_ylabel('Reward')\n",
    "ax.set_title('Episode vs.Reward')\n",
    "\n",
    "ax.plot(list(range(1, len(reward_hist)+1)), reward_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pybullet_envs\n",
    "import pybullet as p\n",
    "import time\n",
    "from gym.wrappers import Monitor\n",
    "gym.logger.set_level(40)\n",
    "\n",
    "actor = tf.keras.models.load_model('./')\n",
    "\n",
    "env = NormalizedEnv(gym.make('HalfCheetahBulletEnv-v0'))\n",
    "env.spec.timestep_limit = 9000\n",
    "\n",
    "env = Monitor(env, \"recordings\", video_callable=lambda episode_id: True, force=\"true\")\n",
    "\n",
    "env.render(mode=\"human\")\n",
    "observation = env.reset()\n",
    "\n",
    "\n",
    "total_reward = 0\n",
    "for t in range(10000):\n",
    "    # Select action from policy and noise\n",
    "    action = actor(np.array([observation,]))[0]\n",
    "\n",
    "    # Execute action and observe reward/new state\n",
    "    new_observation, reward, done, info = env.step(tf.make_ndarray(action))\n",
    "    env.render(mode=\"human\")\n",
    "    total_reward += reward\n",
    "    if done:\n",
    "        print(\"here\")\n",
    "        break\n",
    "    observation = new_observation\n",
    "print('Total reward: {}'.format(total_reward))\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1. 1.]\n",
      "[-1. -1. -1. -1. -1. -1.]\n",
      "[inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf inf\n",
      " inf inf inf inf inf inf inf inf]\n",
      "[-inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf\n",
      " -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf -inf]\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import pybullet_envs\n",
    "env = gym.make('HalfCheetahBulletEnv-v0')\n",
    "print(env.action_space.high)\n",
    "print(env.action_space.low)\n",
    "print(env.observation_space.high)\n",
    "print(env.observation_space.low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
