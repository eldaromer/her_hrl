{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project\n",
    "\n",
    "## Scott Scheraga  7/24/2020\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Worker\n",
      "Worker\n",
      "Worker\n",
      "Worker\n",
      "Worker\n",
      "Worker\n",
      "Worker\n",
      "Worker\n",
      "Worker\n",
      "Worker\n",
      "Worker\n",
      "Worker\n",
      "Worker\n",
      "Worker\n",
      "Worker\n",
      "Worker"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WorkerWorkerWorker\n",
      "\n",
      "\n",
      "\n",
      "Worker\n"
     ]
    }
   ],
   "source": [
    "#Code is largely built off of https://keras.io/examples/rl/ddpg_pendulum/\n",
    "#HER code is inspired by pybullet code at https://github.com/buntyke/her/blob/master/ddpg_her.py\n",
    "\n",
    "#from gym.envs.registration import registry, make, spec, register\n",
    "#python -m pybullet_envs.examples.enjoy_TF_HalfCheetahBulletEnv_v0_2017may\n",
    "\n",
    "\n",
    "import gym\n",
    "import pybullet_envs\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from gym import wrappers\n",
    "from IPython import display\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "%matplotlib inline\n",
    "import math \n",
    "\n",
    "import os\n",
    "import gym\n",
    "from gym import utils\n",
    "from gym.envs import mujoco\n",
    "import mujoco_py\n",
    "import cv2\n",
    "from gym.envs.robotics import fetch_env\n",
    "import threading\n",
    "threading.activeCount()\n",
    "\n",
    "\n",
    "import multiprocessing\n",
    "\n",
    "def worker():\n",
    "    \"\"\"worker function\"\"\"\n",
    "    print ('Worker')\n",
    "    return\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    jobs = []\n",
    "    for i in range(20):\n",
    "        p = multiprocessing.Process(target=worker)\n",
    "        jobs.append(p)\n",
    "        p.start()\n",
    "        \n",
    "threading.activeCount()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obs space\n",
      "Dict(achieved_goal:Box(3,), desired_goal:Box(3,), observation:Box(25,))\n",
      "OrderedDict([('achieved_goal', array([ 0.80702317, -0.41659766,  1.3797944 ], dtype=float32)), ('desired_goal', array([ 0.96165097, -0.1151522 , -0.32118118], dtype=float32)), ('observation', array([ 0.47664648,  0.05384037, -1.0257145 ,  1.2507931 ,  0.09884124,\n",
      "       -0.8271967 ,  1.5255164 ,  0.20657344, -0.23348081,  1.3605086 ,\n",
      "       -1.6013582 ,  0.1732245 ,  1.7306662 , -0.13927211,  0.05484232,\n",
      "       -0.69705874,  0.9848244 , -0.21922173, -0.5423336 ,  0.6368394 ,\n",
      "       -1.5924684 ,  0.09700383, -0.21450408,  0.5101458 , -1.7884041 ],\n",
      "      dtype=float32))])\n",
      " \n",
      " \n",
      "Action space\n",
      "Box(4,)\n",
      "[-0.7104479   0.69475764  0.15510811 -0.8451068 ]\n",
      "numgoals= 3\n",
      " \n",
      " \n",
      "Size of State Space ->  25\n",
      "Size of Action Space ->  4\n",
      "Max Value of Action ->  1.0\n",
      "Min Value of Action ->  -1.0\n",
      " \n",
      " \n",
      " \n"
     ]
    }
   ],
   "source": [
    "\n",
    "#env = gym.make('Pendulum-v0')\n",
    "#env = gym.make('CartPole-v1')\n",
    "#env = gym.make('HalfCheetahBulletEnv-v0')\n",
    "\n",
    "\n",
    "#env = gym.make('FetchPickAndPlace-v1')\n",
    "\n",
    "\n",
    "env = gym.make('FetchPush-v1')\n",
    "#env = gym.make('FetchReach-v1')\n",
    "\n",
    "#env = gym.make('Reacher-v2')\n",
    "\n",
    "#Env information at:\n",
    "# https://medium.com/@Amritpal001/intro-to-robotics-in-openai-fetch-reach-env-automating-robotics-with-reinforcement-learning-part-2b7452f3a5e9\n",
    "\n",
    "print(\"Obs space\")\n",
    "print(env.observation_space) #.shape\n",
    "print(env.observation_space.sample())\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "print(\"Action space\")\n",
    "print(env.action_space) #.shape\n",
    "print(env.action_space.sample())\n",
    "num_states = env.observation_space['observation'].shape[0]\n",
    "num_goals = env.observation_space['achieved_goal'].shape[0]\n",
    "print(\"numgoals=\",num_goals)\n",
    "\n",
    "#num_states = env.observation_space.shape[0]\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "#num_states=25\n",
    "print(\"Size of State Space ->  {}\".format(num_states))\n",
    "num_actions = env.action_space.shape[0]\n",
    "print(\"Size of Action Space ->  {}\".format(num_actions))\n",
    "\n",
    "upper_bound = env.action_space.high[0]\n",
    "lower_bound = env.action_space.low[0]\n",
    "\n",
    "print(\"Max Value of Action ->  {}\".format(upper_bound))\n",
    "print(\"Min Value of Action ->  {}\".format(lower_bound))\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "print(\" \")\n",
    "#print(\"initial_state\")\n",
    "#print(env.initial_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfrom gym import utils\\nfrom gym.envs.robotics import fetch_env\\n\\n\\n# Ensure we get the path separator correct on windows\\nMODEL_XML_PATH = os.path.join('fetch', 'reach.xml')\\n\\n\\nclass FetchReachEnv(fetch_env.FetchEnv, utils.EzPickle):\\n    def __init__(self, reward_type='sparse'):\\n        initial_qpos = {\\n            'robot0:slide0': 0.4049,\\n            'robot0:slide1': 0.48,\\n            'robot0:slide2': 0.0,\\n        }\\n        fetch_env.FetchEnv.__init__(\\n            self, MODEL_XML_PATH, has_object=False, block_gripper=True, n_substeps=20,\\n            gripper_extra_height=0.2, target_in_the_air=True, target_offset=0.0,\\n            obj_range=0.15, target_range=0.15, distance_threshold=0.05,\\n            initial_qpos=initial_qpos, reward_type=reward_type)\\n        utils.EzPickle.__init__(self)\\n        \\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fetch reach definition\n",
    "#https://github.com/openai/gym/blob/master/gym/envs/robotics/fetch/reach.py\n",
    "\"\"\"\n",
    "from gym import utils\n",
    "from gym.envs.robotics import fetch_env\n",
    "\n",
    "\n",
    "# Ensure we get the path separator correct on windows\n",
    "MODEL_XML_PATH = os.path.join('fetch', 'reach.xml')\n",
    "\n",
    "\n",
    "class FetchReachEnv(fetch_env.FetchEnv, utils.EzPickle):\n",
    "    def __init__(self, reward_type='sparse'):\n",
    "        initial_qpos = {\n",
    "            'robot0:slide0': 0.4049,\n",
    "            'robot0:slide1': 0.48,\n",
    "            'robot0:slide2': 0.0,\n",
    "        }\n",
    "        fetch_env.FetchEnv.__init__(\n",
    "            self, MODEL_XML_PATH, has_object=False, block_gripper=True, n_substeps=20,\n",
    "            gripper_extra_height=0.2, target_in_the_air=True, target_offset=0.0,\n",
    "            obj_range=0.15, target_range=0.15, distance_threshold=0.05,\n",
    "            initial_qpos=initial_qpos, reward_type=reward_type)\n",
    "        utils.EzPickle.__init__(self)\n",
    "        \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OUActionNoise:\n",
    "    def __init__(self, mean, std_deviation, theta=0.15, dt=1e-2, x_initial=None):\n",
    "        self.theta = theta\n",
    "        self.mean = mean\n",
    "        self.std_dev = std_deviation\n",
    "        self.dt = dt\n",
    "        self.x_initial = x_initial\n",
    "        self.reset()\n",
    "\n",
    "    def __call__(self):\n",
    "        # Formula taken from https://www.wikipedia.org/wiki/Ornstein-Uhlenbeck_process.\n",
    "        x = (\n",
    "            self.x_prev\n",
    "            + self.theta * (self.mean - self.x_prev) * self.dt\n",
    "            + self.std_dev * np.sqrt(self.dt) * np.random.normal(size=self.mean.shape)\n",
    "        )\n",
    "        # Store x into x_prev\n",
    "        # Makes next noise dependent on current one\n",
    "        self.x_prev = x\n",
    "        return x\n",
    "\n",
    "    def reset(self):\n",
    "        if self.x_initial is not None:\n",
    "            self.x_prev = self.x_initial\n",
    "        else:\n",
    "            self.x_prev = np.zeros_like(self.mean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Buffer:\n",
    "    def __init__(self, buffer_capacity=100000, batch_size=256):\n",
    "    \n",
    "        # Number of \"experiences\" to store at max\n",
    "        self.buffer_capacity = buffer_capacity\n",
    "        # Num of tuples to train on.\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # Its tells us num of times record() was called.\n",
    "        self.buffer_counter = 0\n",
    "\n",
    "        # Instead of list of tuples as the exp.replay concept go\n",
    "        # We use different np.arrays for each tuple element\n",
    "        self.state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
    "        self.action_buffer = np.zeros((self.buffer_capacity, num_actions))\n",
    "        self.reward_buffer = np.zeros((self.buffer_capacity, 1))\n",
    "        self.next_state_buffer = np.zeros((self.buffer_capacity, num_states))\n",
    "        self.achieved_goal_buffer = np.zeros((self.buffer_capacity, num_goals))\n",
    "        self.goal_buffer = np.zeros((self.buffer_capacity, num_goals)) \n",
    "\n",
    "    # Takes (s,a,r,s') obervation tuple as input\n",
    "    def record(self, obs_tuple):\n",
    "        # Set index to zero if buffer_capacity is exceeded,\n",
    "        # replacing old records\n",
    "        index = self.buffer_counter % self.buffer_capacity\n",
    "\n",
    "        self.state_buffer[index] = obs_tuple[0]\n",
    "        self.action_buffer[index] = obs_tuple[1]\n",
    "        self.reward_buffer[index] = obs_tuple[2]\n",
    "        self.next_state_buffer[index] = obs_tuple[3]\n",
    "        self.achieved_goal_buffer[index] = obs_tuple[4]\n",
    "        self.goal_buffer[index] = obs_tuple[5]\n",
    "        \n",
    "        \n",
    "        if self.buffer_counter<self.buffer_capacity:\n",
    "            self.buffer_counter += 1\n",
    "            \n",
    "\n",
    "    # We compute the loss and update parameters\n",
    "    def learn(self):\n",
    "        # Get sampling range\n",
    "        record_range = min(self.buffer_counter, self.buffer_capacity)\n",
    "        # Randomly sample indices\n",
    "        batch_indices = np.random.choice(record_range, self.batch_size)\n",
    "\n",
    "        # Convert to tensors\n",
    "        state_batch = tf.convert_to_tensor(self.state_buffer[batch_indices])\n",
    "        action_batch = tf.convert_to_tensor(self.action_buffer[batch_indices])\n",
    "        reward_batch = tf.convert_to_tensor(self.reward_buffer[batch_indices])\n",
    "        reward_batch = tf.cast(reward_batch, dtype=tf.float32)\n",
    "        next_state_batch = tf.convert_to_tensor(self.next_state_buffer[batch_indices])\n",
    "        achieved_goal_batch = tf.convert_to_tensor(self.achieved_goal_buffer[batch_indices])\n",
    "        goal_batch = tf.convert_to_tensor(self.goal_buffer[batch_indices])\n",
    "\n",
    "        # Training and updating Actor & Critic networks.\n",
    "        # See Pseudo Code.\n",
    "        with tf.GradientTape() as tape:\n",
    "            target_actions = target_actor(next_state_batch)\n",
    "            y = reward_batch + gamma * target_critic([next_state_batch, target_actions])\n",
    "            critic_value = critic_model([state_batch, action_batch])\n",
    "            critic_loss = tf.math.reduce_mean(tf.math.square(y - critic_value))\n",
    "\n",
    "        critic_grad = tape.gradient(critic_loss, critic_model.trainable_variables)\n",
    "        critic_optimizer.apply_gradients(\n",
    "            zip(critic_grad, critic_model.trainable_variables)\n",
    "        )\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            actions = actor_model(state_batch)\n",
    "            critic_value = critic_model([state_batch, actions])\n",
    "            # Used `-value` as we want to maximize the value given\n",
    "            # by the critic for our actions\n",
    "            actor_loss = -tf.math.reduce_mean(critic_value)\n",
    "\n",
    "        actor_grad = tape.gradient(actor_loss, actor_model.trainable_variables)\n",
    "        actor_optimizer.apply_gradients(\n",
    "            zip(actor_grad, actor_model.trainable_variables)\n",
    "        )\n",
    "\n",
    "\n",
    "# This update target parameters slowly\n",
    "# Based on rate `tau`, which is much less than one.\n",
    "def update_target(tau):\n",
    "    new_weights = []\n",
    "    target_variables = target_critic.weights\n",
    "    for i, variable in enumerate(critic_model.weights):\n",
    "        new_weights.append(variable * tau + target_variables[i] * (1 - tau))\n",
    "\n",
    "    target_critic.set_weights(new_weights)\n",
    "\n",
    "    new_weights = []\n",
    "    target_variables = target_actor.weights\n",
    "    for i, variable in enumerate(actor_model.weights):\n",
    "        new_weights.append(variable * tau + target_variables[i] * (1 - tau))\n",
    "\n",
    "    target_actor.set_weights(new_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "layersize=400\n",
    "\n",
    "def get_actor():  #makes actor network\n",
    "    # Initialize weights between -3e-3 and 3-e3\n",
    "    last_init = tf.random_uniform_initializer(minval=-0.003, maxval=0.003)\n",
    "\n",
    "    inputs = layers.Input(shape=(num_states,))\n",
    "    out = layers.Dense(layersize, activation=\"relu\")(inputs)\n",
    "    out = layers.BatchNormalization()(out)\n",
    "    out = layers.Dense(layersize, activation=\"relu\")(out)\n",
    "    out = layers.BatchNormalization()(out)\n",
    "    outputs = layers.Dense(num_actions, activation=\"tanh\", kernel_initializer=last_init)(out)\n",
    "    # Our upper bound is 2.0 for Pendulum.\n",
    "    outputs = outputs * upper_bound\n",
    "    model = tf.keras.Model(inputs, outputs)\n",
    "    return model\n",
    " \n",
    "def get_critic():\n",
    "    # State as input\n",
    "    state_input = layers.Input(shape=(num_states))\n",
    "    #state_out = layers.Dense(16, activation=\"relu\")(state_input)#Changed from 16 to 512 in V5\n",
    "    #state_out = layers.BatchNormalization()(state_out)\n",
    "    #state_out = layers.Dense(32, activation=\"relu\")(state_out)#Changed from 32 to 512 in V5\n",
    "    #state_out = layers.BatchNormalization()(state_out) \n",
    "\n",
    "    # Action as input\n",
    "    action_input = layers.Input(shape=(num_actions))\n",
    "    #action_out = layers.Dense(32, activation=\"relu\")(action_input)  #Changed from 32 to 512 in V5\n",
    "    #action_out = layers.BatchNormalization()(action_out)#Changed from 32 to 512 in V5\n",
    "\n",
    " \n",
    "    # Both are passed through seperate layer before concatenating\n",
    "    #concat = layers.Concatenate()([state_out, action_out])\n",
    "    concat = layers.Concatenate()([state_input, action_input])\n",
    "\n",
    "    out = layers.Dense(layersize, activation=\"relu\")(concat)\n",
    "    out = layers.BatchNormalization()(out)\n",
    "    out = layers.Dense(layersize, activation=\"relu\")(out)\n",
    "    out = layers.BatchNormalization()(out)\n",
    "    outputs = layers.Dense(1)(out)\n",
    "\n",
    "    # Outputs single value for give state-action\n",
    "    model = tf.keras.Model([state_input, action_input], outputs)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy(state, noise_object):\n",
    "    sampled_actions = tf.squeeze(actor_model(state))\n",
    "    noise = noise_object()\n",
    "    # Adding noise to action\n",
    "    sampled_actions = sampled_actions.numpy() + noise\n",
    "\n",
    "    # We make sure action is within bounds\n",
    "    legal_action = np.clip(sampled_actions, lower_bound, upper_bound)\n",
    "\n",
    "    return np.squeeze(legal_action)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "std_dev = 0.2\n",
    "ou_noise = OUActionNoise(mean=np.zeros(1), std_deviation=float(std_dev) * np.ones(1))\n",
    "\n",
    "#actor_model = get_actor()\n",
    "#critic_model = get_critic()\n",
    "\n",
    "#target_actor = get_actor()\n",
    "#target_critic = get_critic()\n",
    "#print(actor_model.summary() )\n",
    "#critic_model.summary() \n",
    "\n",
    "# Making the weights equal initially\n",
    "\n",
    "\n",
    "# Learning rate for actor-critic models\n",
    "critic_lr = 0.001  #originally .002\n",
    "actor_lr = 0.001  #originally .001\n",
    "\n",
    "\n",
    "#total_episodes = 100\n",
    "# Discount factor for future rewards\n",
    "gamma = 0.99\n",
    "# Used to update target networks\n",
    "tau = 0.01\n",
    "\n",
    "\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time is:  2020-07-24 01:10:50.340263\n",
      "Episode * 0 *  Episodic Reward: -50.0\n",
      "Episode * 100 *  Episodic Reward: -50.0\n",
      "Episode * 200 *  Episodic Reward: -50.0\n",
      "Episode * 300 *  Episodic Reward: -50.0\n",
      "Episode * 400 *  Episodic Reward: -50.0\n",
      "Episode * 500 *  Episodic Reward: -50.0\n",
      "Episode * 600 *  Episodic Reward: -50.0\n",
      "Episode * 700 *  Episodic Reward: -50.0\n",
      "Episode * 800 *  Episodic Reward: -50.0\n",
      "Episode * 900 *  Episodic Reward: -50.0\n",
      "Episode * 1000 *  Episodic Reward: -50.0\n",
      "Episode * 1100 *  Episodic Reward: -50.0\n",
      "Episode * 1200 *  Episodic Reward: -50.0\n",
      "Episode * 1300 *  Episodic Reward: -50.0\n",
      "Episode * 1400 *  Episodic Reward: -50.0\n",
      "Episode * 1500 *  Episodic Reward: -50.0\n",
      "Episode * 1600 *  Episodic Reward: -50.0\n",
      "Episode * 1700 *  Episodic Reward: -50.0\n",
      "Episode * 1800 *  Episodic Reward: -50.0\n",
      "Episode * 1900 *  Episodic Reward: -50.0\n",
      "Episode * 2000 *  Episodic Reward: -50.0\n",
      "Episode * 2100 *  Episodic Reward: -50.0\n",
      "Episode * 2200 *  Episodic Reward: -50.0\n",
      "Episode * 2300 *  Episodic Reward: -50.0\n",
      "Episode * 2400 *  Episodic Reward: -50.0\n",
      "Episode * 2500 *  Episodic Reward: -50.0\n",
      "Episode * 2600 *  Episodic Reward: -50.0\n",
      "Episode * 2700 *  Episodic Reward: -50.0\n",
      "Episode * 2800 *  Episodic Reward: -50.0\n",
      "Episode * 2900 *  Episodic Reward: -50.0\n",
      "Episode * 3000 *  Episodic Reward: -50.0\n",
      "Episode * 3100 *  Episodic Reward: -50.0\n",
      "Episode * 3200 *  Episodic Reward: -50.0\n",
      "Episode * 3300 *  Episodic Reward: -50.0\n",
      "Episode * 3400 *  Episodic Reward: -50.0\n",
      "Episode * 3500 *  Episodic Reward: -50.0\n",
      "Episode * 3600 *  Episodic Reward: -50.0\n",
      "Episode * 3700 *  Episodic Reward: -50.0\n",
      "Episode * 3800 *  Episodic Reward: -50.0\n",
      "Episode * 3900 *  Episodic Reward: -50.0\n",
      "Episode * 4000 *  Episodic Reward: -50.0\n",
      "Episode * 4100 *  Episodic Reward: -50.0\n",
      "Episode * 4200 *  Episodic Reward: -50.0\n",
      "Episode * 4300 *  Episodic Reward: -50.0\n",
      "Episode * 4400 *  Episodic Reward: -50.0\n",
      "Episode * 4500 *  Episodic Reward: -50.0\n",
      "Episode * 4600 *  Episodic Reward: -50.0\n",
      "Episode * 4700 *  Episodic Reward: -50.0\n",
      "Episode * 4800 *  Episodic Reward: -50.0\n",
      "Episode * 4900 *  Episodic Reward: -50.0\n",
      "Episode * 5000 *  Episodic Reward: -50.0\n",
      "Episode * 5100 *  Episodic Reward: -50.0\n",
      "Episode * 5200 *  Episodic Reward: -50.0\n",
      "Episode * 5300 *  Episodic Reward: -50.0\n",
      "Episode * 5400 *  Episodic Reward: -50.0\n",
      "Episode * 5500 *  Episodic Reward: -50.0\n",
      "Episode * 5600 *  Episodic Reward: -50.0\n",
      "Episode * 5700 *  Episodic Reward: -50.0\n",
      "Episode * 5800 *  Episodic Reward: -50.0\n",
      "Episode * 5900 *  Episodic Reward: -50.0\n",
      "Episode * 6000 *  Episodic Reward: -50.0\n",
      "Episode * 6100 *  Episodic Reward: -50.0\n",
      "Episode * 6200 *  Episodic Reward: -50.0\n",
      "Episode * 6300 *  Episodic Reward: -50.0\n",
      "Episode * 6400 *  Episodic Reward: -50.0\n",
      "Episode * 6500 *  Episodic Reward: -50.0\n",
      "Episode * 6600 *  Episodic Reward: 0.0\n",
      "Episode * 6700 *  Episodic Reward: -50.0\n",
      "Episode * 6800 *  Episodic Reward: -50.0\n",
      "Episode * 6900 *  Episodic Reward: -50.0\n",
      "Episode * 7000 *  Episodic Reward: -50.0\n",
      "Episode * 7100 *  Episodic Reward: -50.0\n",
      "Episode * 7200 *  Episodic Reward: -50.0\n",
      "Episode * 7300 *  Episodic Reward: -50.0\n",
      "Episode * 7400 *  Episodic Reward: -50.0\n",
      "Episode * 7500 *  Episodic Reward: -50.0\n",
      "Episode * 7600 *  Episodic Reward: -50.0\n",
      "Episode * 7700 *  Episodic Reward: -50.0\n",
      "Episode * 7800 *  Episodic Reward: -50.0\n",
      "Episode * 7900 *  Episodic Reward: -50.0\n",
      "Episode * 8000 *  Episodic Reward: -50.0\n",
      "Episode * 8100 *  Episodic Reward: -50.0\n",
      "Episode * 8200 *  Episodic Reward: 0.0\n",
      "Episode * 8300 *  Episodic Reward: -50.0\n",
      "Episode * 8400 *  Episodic Reward: -50.0\n",
      "Episode * 8500 *  Episodic Reward: -50.0\n",
      "Episode * 8600 *  Episodic Reward: -50.0\n",
      "Episode * 8700 *  Episodic Reward: -50.0\n",
      "Episode * 8800 *  Episodic Reward: -50.0\n",
      "Episode * 8900 *  Episodic Reward: -50.0\n",
      "Episode * 9000 *  Episodic Reward: -50.0\n",
      "Episode * 9100 *  Episodic Reward: -50.0\n",
      "Episode * 9200 *  Episodic Reward: -50.0\n",
      "Episode * 9300 *  Episodic Reward: -50.0\n",
      "Episode * 9400 *  Episodic Reward: -50.0\n",
      "Episode * 9500 *  Episodic Reward: -50.0\n",
      "Episode * 9600 *  Episodic Reward: -50.0\n",
      "Episode * 9700 *  Episodic Reward: -50.0\n",
      "Episode * 9800 *  Episodic Reward: -50.0\n",
      "Episode * 9900 *  Episodic Reward: -50.0\n",
      "Episode * 10000 *  Episodic Reward: -50.0\n",
      "Episode * 10100 *  Episodic Reward: -50.0\n",
      "Episode * 10200 *  Episodic Reward: -50.0\n",
      "Episode * 10300 *  Episodic Reward: -50.0\n",
      "Episode * 10400 *  Episodic Reward: -50.0\n",
      "Episode * 10500 *  Episodic Reward: -50.0\n",
      "Episode * 10600 *  Episodic Reward: -50.0\n",
      "Episode * 10700 *  Episodic Reward: 0.0\n",
      "Episode * 10800 *  Episodic Reward: -50.0\n",
      "Episode * 10900 *  Episodic Reward: -50.0\n",
      "Episode * 11000 *  Episodic Reward: -50.0\n",
      "Episode * 11100 *  Episodic Reward: -50.0\n",
      "Episode * 11200 *  Episodic Reward: -50.0\n",
      "Episode * 11300 *  Episodic Reward: -50.0\n",
      "Episode * 11400 *  Episodic Reward: -50.0\n",
      "Episode * 11500 *  Episodic Reward: -50.0\n",
      "Episode * 11600 *  Episodic Reward: -50.0\n",
      "Episode * 11700 *  Episodic Reward: -50.0\n",
      "Episode * 11800 *  Episodic Reward: -50.0\n",
      "Episode * 11900 *  Episodic Reward: -50.0\n",
      "Episode * 12000 *  Episodic Reward: -50.0\n",
      "Episode * 12100 *  Episodic Reward: -50.0\n",
      "Episode * 12200 *  Episodic Reward: -50.0\n",
      "Episode * 12300 *  Episodic Reward: -50.0\n",
      "Episode * 12400 *  Episodic Reward: -50.0\n",
      "Episode * 12500 *  Episodic Reward: 0.0\n",
      "Episode * 12600 *  Episodic Reward: -50.0\n",
      "Episode * 12700 *  Episodic Reward: -50.0\n",
      "Episode * 12800 *  Episodic Reward: -50.0\n",
      "Episode * 12900 *  Episodic Reward: 0.0\n",
      "Episode * 13000 *  Episodic Reward: -50.0\n",
      "Episode * 13100 *  Episodic Reward: -50.0\n",
      "Episode * 13200 *  Episodic Reward: -50.0\n",
      "Episode * 13300 *  Episodic Reward: -50.0\n",
      "Episode * 13400 *  Episodic Reward: -50.0\n",
      "Episode * 13500 *  Episodic Reward: -50.0\n",
      "Episode * 13600 *  Episodic Reward: -50.0\n",
      "Episode * 13700 *  Episodic Reward: -50.0\n",
      "Episode * 13800 *  Episodic Reward: 0.0\n",
      "Episode * 13900 *  Episodic Reward: -50.0\n",
      "Episode * 14000 *  Episodic Reward: -50.0\n",
      "Episode * 14100 *  Episodic Reward: -50.0\n",
      "Episode * 14200 *  Episodic Reward: -50.0\n",
      "Episode * 14300 *  Episodic Reward: -50.0\n",
      "Episode * 14400 *  Episodic Reward: -50.0\n",
      "Episode * 14500 *  Episodic Reward: -50.0\n",
      "Episode * 14600 *  Episodic Reward: -50.0\n",
      "Episode * 14700 *  Episodic Reward: -50.0\n",
      "Episode * 14800 *  Episodic Reward: -50.0\n",
      "Episode * 14900 *  Episodic Reward: -50.0\n",
      "Episode * 15000 *  Episodic Reward: -50.0\n",
      "Episode * 15100 *  Episodic Reward: -50.0\n",
      "Episode * 15200 *  Episodic Reward: -50.0\n",
      "Episode * 15300 *  Episodic Reward: -50.0\n",
      "Episode * 15400 *  Episodic Reward: -50.0\n",
      "Episode * 15500 *  Episodic Reward: -50.0\n",
      "Episode * 15600 *  Episodic Reward: -50.0\n",
      "Episode * 15700 *  Episodic Reward: -50.0\n",
      "Episode * 15800 *  Episodic Reward: -50.0\n",
      "Episode * 15900 *  Episodic Reward: -50.0\n",
      "Episode * 16000 *  Episodic Reward: -50.0\n",
      "Episode * 16100 *  Episodic Reward: -50.0\n",
      "Episode * 16200 *  Episodic Reward: -50.0\n",
      "Episode * 16300 *  Episodic Reward: -50.0\n",
      "Episode * 16400 *  Episodic Reward: -50.0\n",
      "Episode * 16500 *  Episodic Reward: -50.0\n",
      "Episode * 16600 *  Episodic Reward: -50.0\n",
      "Episode * 16700 *  Episodic Reward: -50.0\n",
      "Episode * 16800 *  Episodic Reward: 0.0\n",
      "Episode * 16900 *  Episodic Reward: -50.0\n",
      "Episode * 17000 *  Episodic Reward: -50.0\n",
      "Episode * 17100 *  Episodic Reward: -50.0\n",
      "Episode * 17200 *  Episodic Reward: -50.0\n",
      "Episode * 17300 *  Episodic Reward: -50.0\n",
      "Episode * 17400 *  Episodic Reward: -50.0\n",
      "Episode * 17500 *  Episodic Reward: -50.0\n",
      "Episode * 17600 *  Episodic Reward: 0.0\n",
      "Episode * 17700 *  Episodic Reward: -50.0\n",
      "Episode * 17800 *  Episodic Reward: -50.0\n",
      "Episode * 17900 *  Episodic Reward: -50.0\n",
      "Episode * 18000 *  Episodic Reward: -50.0\n",
      "Episode * 18100 *  Episodic Reward: -50.0\n",
      "Episode * 18200 *  Episodic Reward: -50.0\n",
      "Episode * 18300 *  Episodic Reward: -50.0\n",
      "Episode * 18400 *  Episodic Reward: -50.0\n",
      "Episode * 18500 *  Episodic Reward: 0.0\n",
      "Episode * 18600 *  Episodic Reward: -50.0\n",
      "Episode * 18700 *  Episodic Reward: -50.0\n",
      "Episode * 18800 *  Episodic Reward: -50.0\n",
      "Episode * 18900 *  Episodic Reward: -50.0\n",
      "Episode * 19000 *  Episodic Reward: -50.0\n",
      "Episode * 19100 *  Episodic Reward: -50.0\n",
      "Episode * 19200 *  Episodic Reward: -50.0\n",
      "Episode * 19300 *  Episodic Reward: -50.0\n",
      "Episode * 19400 *  Episodic Reward: -50.0\n",
      "Episode * 19500 *  Episodic Reward: -50.0\n",
      "Episode * 19600 *  Episodic Reward: -50.0\n",
      "Episode * 19700 *  Episodic Reward: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode * 19800 *  Episodic Reward: -50.0\n",
      "Episode * 19900 *  Episodic Reward: -50.0\n",
      "Episode * 20000 *  Episodic Reward: -50.0\n",
      "Episode * 20100 *  Episodic Reward: -50.0\n",
      "Episode * 20200 *  Episodic Reward: -50.0\n",
      "Episode * 20300 *  Episodic Reward: -50.0\n",
      "Episode * 20400 *  Episodic Reward: -50.0\n",
      "Episode * 20500 *  Episodic Reward: -50.0\n",
      "Episode * 20600 *  Episodic Reward: -50.0\n",
      "Episode * 20700 *  Episodic Reward: -50.0\n",
      "Episode * 20800 *  Episodic Reward: -50.0\n",
      "Episode * 20900 *  Episodic Reward: -50.0\n",
      "Episode * 21000 *  Episodic Reward: 0.0\n",
      "Episode * 21100 *  Episodic Reward: -50.0\n",
      "Episode * 21200 *  Episodic Reward: -50.0\n",
      "Episode * 21300 *  Episodic Reward: -50.0\n",
      "Episode * 21400 *  Episodic Reward: -50.0\n",
      "Episode * 21500 *  Episodic Reward: -50.0\n",
      "Episode * 21600 *  Episodic Reward: -50.0\n",
      "Episode * 21700 *  Episodic Reward: -50.0\n",
      "Episode * 21800 *  Episodic Reward: -50.0\n",
      "Episode * 21900 *  Episodic Reward: -50.0\n",
      "Episode * 22000 *  Episodic Reward: -50.0\n",
      "Episode * 22100 *  Episodic Reward: -50.0\n",
      "Episode * 22200 *  Episodic Reward: -50.0\n",
      "Episode * 22300 *  Episodic Reward: -50.0\n",
      "Episode * 22400 *  Episodic Reward: -50.0\n",
      "Episode * 22500 *  Episodic Reward: -50.0\n",
      "Episode * 22600 *  Episodic Reward: -50.0\n",
      "Episode * 22700 *  Episodic Reward: -50.0\n",
      "Episode * 22800 *  Episodic Reward: -50.0\n",
      "Episode * 22900 *  Episodic Reward: -50.0\n",
      "Episode * 23000 *  Episodic Reward: -50.0\n",
      "Episode * 23100 *  Episodic Reward: -50.0\n",
      "Episode * 23200 *  Episodic Reward: -50.0\n",
      "Episode * 23300 *  Episodic Reward: -50.0\n",
      "Episode * 23400 *  Episodic Reward: -50.0\n",
      "Episode * 23500 *  Episodic Reward: -50.0\n",
      "Episode * 23600 *  Episodic Reward: -50.0\n",
      "Episode * 23700 *  Episodic Reward: -50.0\n",
      "Episode * 23800 *  Episodic Reward: -50.0\n",
      "Episode * 23900 *  Episodic Reward: -50.0\n",
      "Episode * 24000 *  Episodic Reward: -50.0\n",
      "Episode * 24100 *  Episodic Reward: -50.0\n",
      "Episode * 24200 *  Episodic Reward: -50.0\n",
      "Episode * 24300 *  Episodic Reward: -50.0\n",
      "Episode * 24400 *  Episodic Reward: -50.0\n",
      "Episode * 24500 *  Episodic Reward: -50.0\n",
      "Episode * 24600 *  Episodic Reward: -50.0\n",
      "Episode * 24700 *  Episodic Reward: -50.0\n",
      "Episode * 24800 *  Episodic Reward: -50.0\n",
      "Episode * 24900 *  Episodic Reward: -50.0\n",
      "Episode * 25000 *  Episodic Reward: -50.0\n",
      "Episode * 25100 *  Episodic Reward: -50.0\n",
      "Episode * 25200 *  Episodic Reward: -50.0\n",
      "Episode * 25300 *  Episodic Reward: -50.0\n",
      "Episode * 25400 *  Episodic Reward: -50.0\n",
      "Episode * 25500 *  Episodic Reward: -50.0\n",
      "Episode * 25600 *  Episodic Reward: -50.0\n",
      "Episode * 25700 *  Episodic Reward: -50.0\n",
      "Episode * 25800 *  Episodic Reward: 0.0\n",
      "Episode * 25900 *  Episodic Reward: -50.0\n",
      "Episode * 26000 *  Episodic Reward: -50.0\n",
      "Episode * 26100 *  Episodic Reward: -50.0\n",
      "Episode * 26200 *  Episodic Reward: -50.0\n",
      "Episode * 26300 *  Episodic Reward: -50.0\n",
      "Episode * 26400 *  Episodic Reward: -50.0\n",
      "Episode * 26500 *  Episodic Reward: -50.0\n",
      "Episode * 26600 *  Episodic Reward: -50.0\n",
      "Episode * 26700 *  Episodic Reward: -50.0\n",
      "Episode * 26800 *  Episodic Reward: -50.0\n",
      "Episode * 26900 *  Episodic Reward: -50.0\n",
      "Episode * 27000 *  Episodic Reward: -50.0\n",
      "Episode * 27100 *  Episodic Reward: -50.0\n",
      "Episode * 27200 *  Episodic Reward: 0.0\n",
      "Episode * 27300 *  Episodic Reward: -50.0\n",
      "Episode * 27400 *  Episodic Reward: -50.0\n",
      "Episode * 27500 *  Episodic Reward: -50.0\n",
      "Episode * 27600 *  Episodic Reward: 0.0\n",
      "Episode * 27700 *  Episodic Reward: -50.0\n",
      "Episode * 27800 *  Episodic Reward: -50.0\n",
      "Episode * 27900 *  Episodic Reward: -50.0\n",
      "Episode * 28000 *  Episodic Reward: -50.0\n",
      "Episode * 28100 *  Episodic Reward: -50.0\n",
      "Episode * 28200 *  Episodic Reward: -50.0\n",
      "Episode * 28300 *  Episodic Reward: -50.0\n",
      "Episode * 28400 *  Episodic Reward: -50.0\n",
      "Episode * 28500 *  Episodic Reward: -50.0\n",
      "Episode * 28600 *  Episodic Reward: -50.0\n",
      "Episode * 28700 *  Episodic Reward: -50.0\n",
      "Episode * 28800 *  Episodic Reward: -50.0\n",
      "Episode * 28900 *  Episodic Reward: -50.0\n",
      "Episode * 29000 *  Episodic Reward: -50.0\n",
      "Episode * 29100 *  Episodic Reward: -50.0\n",
      "Episode * 29200 *  Episodic Reward: -50.0\n",
      "Episode * 29300 *  Episodic Reward: -50.0\n",
      "Episode * 29400 *  Episodic Reward: -50.0\n",
      "Episode * 29500 *  Episodic Reward: -50.0\n",
      "Episode * 29600 *  Episodic Reward: -50.0\n",
      "Episode * 29700 *  Episodic Reward: -50.0\n",
      "Episode * 29800 *  Episodic Reward: -50.0\n",
      "Episode * 29900 *  Episodic Reward: -50.0\n",
      "Episode * 30000 *  Episodic Reward: -50.0\n",
      "Episode * 30100 *  Episodic Reward: -50.0\n",
      "Episode * 30200 *  Episodic Reward: -50.0\n",
      "Episode * 30300 *  Episodic Reward: -50.0\n",
      "Episode * 30400 *  Episodic Reward: -50.0\n",
      "Episode * 30500 *  Episodic Reward: -50.0\n",
      "Episode * 30600 *  Episodic Reward: -50.0\n",
      "Episode * 30700 *  Episodic Reward: -50.0\n",
      "Episode * 30800 *  Episodic Reward: -50.0\n",
      "Episode * 30900 *  Episodic Reward: -50.0\n",
      "Episode * 31000 *  Episodic Reward: -50.0\n",
      "Episode * 31100 *  Episodic Reward: -50.0\n",
      "Episode * 31200 *  Episodic Reward: -50.0\n",
      "Episode * 31300 *  Episodic Reward: -50.0\n",
      "Episode * 31400 *  Episodic Reward: -50.0\n",
      "Episode * 31500 *  Episodic Reward: -50.0\n",
      "Episode * 31600 *  Episodic Reward: -50.0\n",
      "Episode * 31700 *  Episodic Reward: -50.0\n",
      "Episode * 31800 *  Episodic Reward: -50.0\n",
      "Episode * 31900 *  Episodic Reward: -50.0\n",
      "Episode * 32000 *  Episodic Reward: 0.0\n",
      "Episode * 32100 *  Episodic Reward: -50.0\n",
      "Episode * 32200 *  Episodic Reward: -50.0\n",
      "Episode * 32300 *  Episodic Reward: -50.0\n",
      "Episode * 32400 *  Episodic Reward: -50.0\n",
      "Episode * 32500 *  Episodic Reward: -50.0\n",
      "Episode * 32600 *  Episodic Reward: -50.0\n",
      "Episode * 32700 *  Episodic Reward: -50.0\n",
      "Episode * 32800 *  Episodic Reward: -50.0\n",
      "Episode * 32900 *  Episodic Reward: -50.0\n",
      "Episode * 33000 *  Episodic Reward: -50.0\n",
      "Episode * 33100 *  Episodic Reward: 0.0\n",
      "Episode * 33200 *  Episodic Reward: -50.0\n",
      "Episode * 33300 *  Episodic Reward: -50.0\n",
      "Episode * 33400 *  Episodic Reward: -46.0\n",
      "Episode * 33500 *  Episodic Reward: -50.0\n",
      "Episode * 33600 *  Episodic Reward: -50.0\n",
      "Episode * 33700 *  Episodic Reward: 0.0\n",
      "Episode * 33800 *  Episodic Reward: -50.0\n",
      "Episode * 33900 *  Episodic Reward: -50.0\n",
      "Episode * 34000 *  Episodic Reward: -50.0\n",
      "Episode * 34100 *  Episodic Reward: -50.0\n",
      "Episode * 34200 *  Episodic Reward: -50.0\n",
      "Episode * 34300 *  Episodic Reward: 0.0\n",
      "Episode * 34400 *  Episodic Reward: -50.0\n",
      "Episode * 34500 *  Episodic Reward: -50.0\n",
      "Episode * 34600 *  Episodic Reward: -50.0\n"
     ]
    },
    {
     "ename": "MujocoException",
     "evalue": "Got MuJoCo Warning: Nan, Inf or huge value in QACC at DOF 0. The simulation is unstable. Time = 0.4000.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMujocoException\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-2de1164cb975>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf_prev_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mou_noise\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;31m# Recieve state and reward from environment.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;31m#desired_goal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/csci-7000-rl/lib/python3.7/site-packages/gym/wrappers/time_limit.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Cannot call env.step() before calling reset()\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_max_episode_steps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/csci-7000-rl/lib/python3.7/site-packages/gym/envs/robotics/robot_env.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhigh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step_callback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_obs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/csci-7000-rl/lib/python3.7/site-packages/mujoco_py/mjsim.pyx\u001b[0m in \u001b[0;36mmujoco_py.cymj.MjSim.step\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/csci-7000-rl/lib/python3.7/site-packages/mujoco_py/cymj.pyx\u001b[0m in \u001b[0;36mmujoco_py.cymj.wrap_mujoco_warning.__exit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/csci-7000-rl/lib/python3.7/site-packages/mujoco_py/cymj.pyx\u001b[0m in \u001b[0;36mmujoco_py.cymj.c_warning_callback\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/csci-7000-rl/lib/python3.7/site-packages/mujoco_py/builder.py\u001b[0m in \u001b[0;36muser_warning_raise_exception\u001b[0;34m(warn_bytes)\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'Unknown warning type'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwarn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mMujocoException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwarn\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'Check for NaN in simulation.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mMujocoException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Got MuJoCo Warning: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMujocoException\u001b[0m: Got MuJoCo Warning: Nan, Inf or huge value in QACC at DOF 0. The simulation is unstable. Time = 0.4000."
     ]
    }
   ],
   "source": [
    "# To store reward history of each episode\n",
    "\n",
    "ep_reward_list = []\n",
    "buffersize=100000\n",
    "buffer = Buffer(buffersize, 128)  \n",
    "actor_model = get_actor()\n",
    "critic_model = get_critic()\n",
    "\n",
    "\n",
    "target_actor = get_actor()\n",
    "target_critic = get_critic()\n",
    "target_actor.set_weights(actor_model.get_weights())\n",
    "target_critic.set_weights(critic_model.get_weights())\n",
    "critic_optimizer = tf.keras.optimizers.Adam(critic_lr)\n",
    "actor_optimizer = tf.keras.optimizers.Adam(actor_lr)\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "\n",
    "HER_active=False\n",
    "#K=4 #K is the ratio of HER buffer length to the regular buffer length\n",
    "\n",
    "\"\"\"\n",
    "actor_model.load_weights(\"chet_actor100v8.h5\")\n",
    "critic_model.load_weights(\"chet_critic100v8.h5\")\n",
    "\n",
    "target_actor.load_weights(\"chet_target_actor100v8.h5\")\n",
    "target_critic.load_weights(\"chet_target_critic100v8.h5\")\n",
    "\n",
    "\n",
    "with open('rewardlist400v5.txt', 'r') as filehandle:\n",
    "    for line in filehandle:\n",
    "        # remove linebreak which is the last character of the string\n",
    "        currentPlace = line[:-1]\n",
    "\n",
    "        # add item to the list\n",
    "        ep_reward_list.append(currentPlace)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "#env.render()\n",
    "#cymj.MjRenderContextOffscreen(self.sim, 0)\n",
    "dateTimeObj = datetime.now()\n",
    "\n",
    "\n",
    "print(\"Time is: \", dateTimeObj)\n",
    "\n",
    "\n",
    "for ep in range(80000):\n",
    "    \n",
    "    if ep % 1000 ==0: \n",
    "        actor_model.save_weights(\"pushNOHER_actor.h5\")\n",
    "        critic_model.save_weights(\"pushNOHER_critic.h5\")\n",
    "\n",
    "        target_actor.save_weights(\"pushNOHER_target_actor.h5\")\n",
    "        target_critic.save_weights(\"pushNOHER_target_critic.h5\")\n",
    "\n",
    "        with open('rewardlistpushNOHER.txt', 'w') as filehandle:\n",
    "            for listitem in ep_reward_list:\n",
    "                filehandle.write('%s\\n' % listitem)\n",
    "                \n",
    "    \"\"\"\"\"\"           \n",
    "    prev_state = env.reset()\n",
    "    \n",
    "    episodic_reward = 0\n",
    "    #if HER_active==True:\n",
    "    #epochbuffer = Buffer(50, 128)\n",
    "    #epochHERbuffer = Buffer(50, 128)\n",
    "    while True:\n",
    "        \n",
    "        #achievedcounter=0\n",
    "        #env.render()\n",
    "\n",
    "        tf_prev_state = tf.expand_dims(tf.convert_to_tensor(prev_state['observation']), 0)\n",
    "        #tf_prev_state = tf.expand_dims(tf.convert_to_tensor(prev_state), 0)\n",
    "        \n",
    "        action = policy(tf_prev_state, ou_noise)\n",
    "        # Recieve state and reward from environment.\n",
    "        state, reward, done, info = env.step(action)\n",
    "\n",
    "        #desired_goal\n",
    "        #achieved_goal\n",
    "        buffer.record((prev_state['observation'] ,action, \n",
    "                       reward, state['observation'],state['achieved_goal'],state['desired_goal']))\n",
    "        \n",
    "        episodic_reward += reward\n",
    "        \n",
    "        \n",
    "        #epochbuffer.record((prev_state['observation'] ,action, \n",
    "        #               reward, state['observation'],state['achieved_goal'],state['desired_goal']))\n",
    "        #buffer.record((prev_state, action, reward, state))\n",
    "  \n",
    "        \n",
    "        # End this episode when `done` is True\n",
    "        \n",
    "        if done:\n",
    "            break \n",
    "    \"\"\"         \n",
    "    if HER_active==True:\n",
    "            #print(\"epochbuffer.buffer_counter=\",epochbuffer.buffer_counter)\n",
    "            tempreward=0\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            for t in range(epochbuffer.buffer_counter):\n",
    "                \n",
    "                for k in range(4):\n",
    "                        #For \"future\" strategy\n",
    "                        strategyindex = np.random.randint(t,epochbuffer.buffer_counter)\n",
    "                        \n",
    "                        #For \"final\" strategy\n",
    "                        #strategyindex = epochbuffer.buffer_counter-1\n",
    "                        \n",
    "                        #For \"random\" strategy\n",
    "                        #strategyindex = np.random.randint(0,epochbuffer.buffer_counter)\n",
    "                        \n",
    "\n",
    "                        stateHER = epochbuffer.state_buffer[t]                 \n",
    "                        actionHER = epochbuffer.action_buffer[t]\n",
    "                        next_stateHER = epochbuffer.next_state_buffer[t]\n",
    "                        achieved_goalHER = epochbuffer.achieved_goal_buffer[t]\n",
    "                        \n",
    "                        goalHER = epochbuffer.achieved_goal_buffer[strategyindex]\n",
    "                        \n",
    "                        rewardHER=env.compute_reward(achieved_goalHER, goalHER, info)\n",
    "                        tempreward+=rewardHER\n",
    "                        buffer.record((stateHER, actionHER, \n",
    "                                         rewardHER, next_stateHER, achieved_goalHER, goalHER))\n",
    "                                    # add new experience to her\n",
    "                        #epochHERbuffer.record((stateHER, actionHER, \n",
    "                                   #      rewardHER, next_stateHER, achieved_goalHER, goalHER))\n",
    "    \"\"\"                     \n",
    "                        \n",
    "            #print(\"HERreward=\",tempreward)            \n",
    "    #epochHERbuffer.learn()\n",
    "            #del HERbuffer  \n",
    "    #del epochbuffer  \n",
    "        \n",
    "    buffer.learn()    \n",
    "    update_target(tau)    \n",
    "    prev_state = state    \n",
    "        \n",
    "    ep_reward_list.append(episodic_reward)\n",
    "\n",
    "    # Mean of last 40 episodes\n",
    "    #avg_reward = np.mean(ep_reward_list[-40:])\n",
    "    if ep % 100 ==0:\n",
    "      #  print(\"Episode * {} *  HER Reward: {} *  Episodic Reward: {}\".format(ep,tempreward, episodic_reward ))\n",
    "        print(\"Episode * {} *  Episodic Reward: {}\".format(ep, episodic_reward ))\n",
    "            \n",
    "    #print(\"buffersize= \",buffer.buffer_counter)\n",
    "    #avg_reward_list.append(avg_reward)\n",
    "    \n",
    "dateTimeObj = datetime.now()\n",
    "print(\"Time is: \", dateTimeObj)\n",
    "\n",
    "# Plotting graph\n",
    "# Episodes versus Avg. Rewards\n",
    "plt.plot(ep_reward_list)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Avg. Episodic Reward\")\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nwith open('rewardlistpicplace.txt', 'w') as filehandle:\\n     for listitem in ep_reward_list:\\n                filehandle.write('%s\\n' % listitem)\\n\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actor_model.save_weights(\"pushNOHER_actor.h5\")\n",
    "critic_model.save_weights(\"pushNOHER_critic.h5\")\n",
    "\n",
    "target_actor.save_weights(\"pushNOHER_target_actor.h5\")\n",
    "target_critic.save_weights(\"pushNOHER_target_critic.h5\")\n",
    "\n",
    "with open('rewardlistpushNOHER.txt', 'w') as filehandle:\n",
    "    for listitem in ep_reward_list:\n",
    "           filehandle.write('%s\\n' % listitem)\n",
    "\n",
    "\"\"\"\n",
    "actor_model.load_weights(\"picplace_actor-noHER.h5\")\n",
    "critic_model.load_weights(\"picplace_critic-noHER.h5\")\n",
    "\n",
    "target_actor.load_weights(\"picplace_target_actor-noHER.h5\")\n",
    "target_critic.load_weights(\"picplace_target_critic-noHER.h5\")\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "with open('rewardlistpicplace.txt', 'w') as filehandle:\n",
    "     for listitem in ep_reward_list:\n",
    "                filehandle.write('%s\\n' % listitem)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sucesses= 0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#actor_model.load_weights(\"chet_actor140v5.h5\")\n",
    "#critic_model.load_weights(\"chet_critic140v5.h5\")\n",
    "\n",
    "#target_actor.load_weights(\"chet_target_actor140v5.h5\")\n",
    "#target_critic.load_weights(\"chet_target_critic140v5.h5\")\n",
    "\n",
    "\n",
    "img = plt.imshow(env.render(mode='rgb_array')) # only call this once\n",
    "success=0\n",
    "for j in range(10):\n",
    "    env.reset()\n",
    "    for j in range(50):\n",
    "        \n",
    "        \n",
    "        img.set_data(env.render(mode='rgb_array')) # just update the data\n",
    "        display.display(plt.gcf())\n",
    "        display.clear_output(wait=True)\n",
    "        \n",
    "        \n",
    "        #env.render()\n",
    "        action = env.action_space.sample()\n",
    "    #env.step(action)  #investigate \n",
    "        state, reward, done, info = env.step(action)\n",
    "        if reward==0:\n",
    "            print(\"reached target at timestep\",i)\n",
    "            success+=1\n",
    "            break\n",
    "print(\"sucesses=\",success)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "actor_model.load_weights(\"chet_actor200v7.h5\")\n",
    "critic_model.load_weights(\"chet_critic200v7.h5\")\n",
    "\n",
    "target_actor.load_weights(\"chet_target_actor200v87.h5\")\n",
    "target_critic.load_weights(\"chet_target_critic200v7.h5\")\n",
    "\n",
    "with open('rewardlist200v8.txt', 'w') as filehandle:\n",
    "    for listitem in ep_reward_list:\n",
    "        filehandle.write('%s\\n' % listitem)\n",
    "\"\"\"      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "ename": "DependencyNotInstalled",
     "evalue": "Found neither the ffmpeg nor avconv executables. On OS X, you can install ffmpeg via `brew install ffmpeg`. On most Ubuntu variants, `sudo apt-get install ffmpeg` should do it. On Ubuntu 14.04, however, you'll need to install avconv with `sudo apt-get install libav-tools`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDependencyNotInstalled\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-177-a48485d44b28>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#env = gym.make('SpaceInvaders-v0')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrappers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMonitor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"./gym-results\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/csci-7000-rl/lib/python3.7/site-packages/gym/wrappers/monitor.py\u001b[0m in \u001b[0;36mreset\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_before_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_after_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/csci-7000-rl/lib/python3.7/site-packages/gym/wrappers/monitor.py\u001b[0m in \u001b[0;36m_after_reset\u001b[0;34m(self, observation)\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats_recorder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mafter_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_video_recorder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;31m# Bump *after* all reset activity has finished\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/csci-7000-rl/lib/python3.7/site-packages/gym/wrappers/monitor.py\u001b[0m in \u001b[0;36mreset_video_recorder\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    207\u001b[0m             \u001b[0menabled\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_video_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         )\n\u001b[0;32m--> 209\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvideo_recorder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcapture_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_close_video_recorder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/csci-7000-rl/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py\u001b[0m in \u001b[0;36mcapture_frame\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    114\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_encode_ansi_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_encode_image_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/csci-7000-rl/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py\u001b[0m in \u001b[0;36m_encode_image_frame\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m    160\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_encode_image_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImageEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframes_per_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoder_version'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/csci-7000-rl/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, output_path, frame_shape, frames_per_sec)\u001b[0m\n\u001b[1;32m    253\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'ffmpeg'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDependencyNotInstalled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\"Found neither the ffmpeg nor avconv executables. On OS X, you can install ffmpeg via `brew install ffmpeg`. On most Ubuntu variants, `sudo apt-get install ffmpeg` should do it. On Ubuntu 14.04, however, you'll need to install avconv with `sudo apt-get install libav-tools`.\"\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDependencyNotInstalled\u001b[0m: Found neither the ffmpeg nor avconv executables. On OS X, you can install ffmpeg via `brew install ffmpeg`. On most Ubuntu variants, `sudo apt-get install ffmpeg` should do it. On Ubuntu 14.04, however, you'll need to install avconv with `sudo apt-get install libav-tools`."
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "from IPython import display\n",
    "%matplotlib inline\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "env.reset()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
